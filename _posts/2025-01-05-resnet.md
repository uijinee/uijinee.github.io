---
title: "[Paper Review] Deep Residual Learning for Image Recognition"
date: 2025-01-05 10:00:00 +0900
categories: ["Paper Review", "Supervised Learning"]
tags: ["paper review", "resnet"]
use_math: true
---

# Source

> - CVPR 2016 [Paper](https://arxiv.org/pdf/1512.03385), [Git](https://github.com/KaimingHe/deep-residual-networks)<br>
> - Kaiming He, Xiang
> - 10 Dec 2015
> - (UC San Diego, Facebook AI Research)

---
# Contents
## 0. Abstract

깊은 신경망은 학습하기가 더 어렵다. 우리는 기존의 network보다 더 깊은 network를 쉽게 학습하기 위한 residual learning framework를 제안한다. 우리는 layer를 layer inputs을 참조하는 residual function을 학습하도록 명시적으로 재구성한다. 우리는 이러한 residual network가 최적화하기 쉽고 depth를 증가시켜도 정확도를 향상시킬 수 있음을 실험적으로 증명하였다. 우리는 ImageNet dataset에서 152-layer를 사용해 residual net을 평가하였다. 이는 VGG보다 8배 깊지만 Complexity는 더 낮다. 이 residual net을 앙상블한 결과 ImageNet test set에서 3.57%의 error를 달성하였다. 이 결과는 ILSVRC 2015 Classification task에서 1등을 달성하였다. 우리는 또한 CIFAR-10에서 100개의 Layer와 1000개의 Layer에 대한 분석도 진행하였다.
 
Representatoin의 깊이는 많은 Visual recognition task에서 중요한 역할을 한다. 오로지 우리의 매우 깊은 표현력 덕분에 COCO Object Detection dataset에서 28%의 상대적 개선효과를 얻었다. deep residual nets은 우리의 ILSVRC & COCO 2015 대회회의 제출물에 기초가 되었고 ImageNet Detection, ImageNet Localization, COCO Detection, COCO Segmentation에서 1위를 차지하였다.

---

## 1. Introduction

Deep Convolutional 신경망은 Image Classification에서 일련의 돌파구를 만들어 냈다. Deep network는 자연스럽게 low/mid/high level feature와 Classifier를 통합한다. 그리고 feature의 수준은 쌓여진 layer의 수(depth)에 의해 풍부해진다. 최근의 연구는 network의 깊이가 매우 중요하다는 것을 보여주고 ImageNet 데이터셋 대회에서 좋은 기록들은 16~30개의 매우 깊은 모델을 활용하였다. 또한 많은 다른 복잡한 visual recognition task도 깊은 모델을 통해 큰 혜택을 받았다.

깊이의 중요성을 생각해 볼 때, "더 나은 network를 학습시키는 것이 layer를 쌓는 것만큼 쉬울까?" 라는 질문이 생긴다. 이 질문에 답하는데 방해가 되는 문제는 학습 초기에 수렴을 방해하는 gradient vanishing/exploding 문제이다. 그러나 이 문제는 initialization과 batch normalization을 통해 해결되어 왔고, 이는 수십 층을 가진 network가 SGD를 통한 역전파로 수렴할 수 있게 만들었다.

![alt text](/assets/img/post/paper_review/resnet_degrade.png)

더 깊은 network가 수렴하기 시작하면서 성능 저하 문제(degradation, underfitting을 말하는 듯함)가 발생하였다. network의 깊이가 증가하면서 정확도는 saturated되었고, 성능 저하가 급격히 발생하였다. 놀랍게도 이러한 성능 저하는 overfitting때문에 발생한 것은 아니고 적당히 깊은 모델에 layer를 더 추가하는 것은 더 높은 training error를 발생시켰다. 위의 그림은 이의 전형적인 예시이다.

훈련 과정에서 성능 저하는 모든 system이 동일하게 최적화하기 쉬운 것은 아니라는 것을 나타낸다. 얕은 architecture와 여기에 layer를 추가한 깊은 architecture를 생각해보자. 이 깊은 모델의 Solution은 다음과 같다. 얕은 모델에서 학습된 layer들을 그대로 복사하고 이 위에 identity mapping layer를 추가하는 것이다. 위와 같이 구성된 Solution의 경우 깊은 모델이 얕은 모델보다 더 높은 training error를 만들어서는 안된다는 것을 의미한다. 그러나 실험에 따르면 현재 기존의 최적화 방법들은 이론적으로 존재하는 최적의 해를 찾을 수 없다는 것을 보여준다.

이 논문에서 우리는 deep residual learning framework를 통해 성능 저하 문제를 다룰 것이다. 몇몇 적은 수의 layer가 underlying mapping(indentity function을 말하는 듯 함)을 학습하는 대신에, 우리는 residual mapping을 학습하도록 명시적으로 설계한다. 목표가 되는 mapping function을 $H(x)$로 표시하면 우리는 Layer가 $F(x):=H(X) - X$라는 mapping function을 학습하도록 설계한다. 즉, 원래의 mapping function은 $F(x) + x$로 재구성 된다. 우리는 이 residual mapping($F(x) + x$)을 최적화하는 것이 기존의 unreferenced mapping$F(x)$을 최적화하는 것보다 더 쉽다고 가설을 세운다. 극단적으로 생각해 보면 identity mapping을 학습해야 한다고 할 때, $F(x) + x$를 0으로 만드는 것이 $F(x)$를 x으로 만드는 것보다 더 쉽다. 

> 즉, 바로 위 문단에서 이미 학습된 Shallow network에 추가적인 layer를 쌓았으니까, 우리가 생각할 수 있는 Deep network가 최적의 결과를 내는 방법은 적어도 추가적인 layer가 identity function이면 된다. 하지만 실제 실험 결과에서는 Deep network가 Shallow network보다 더 낮은 성능을 보였으므로 identity function을 학습하지 못했음을 알 수 있다. 이를 해결하기 위해 위와 같은 idea를 생각해낸 것 같다.

![alt text](/assets/img/post/paper_review/shortcut_connection.png)

이 $F(x) + x$ 수식은 위와 같이 "Shortcut connections"를 가진 feedfoward 신경망으로 구현할 수 있다. shortcut connection은 1개/n개의 layer를 skip하는 것을 말한다. 우리의 경우 이 shortcut connection이 단순히 identity mapping을 수행하고, 이 output은 이후 stacked layer의 output과 더해진다. 이는 추가적인 parameter나 계산복잡도 없이 구현할 수 있다. 전체적인 network는 SGD를 사용한 역전파로 end-to-end로 학습 가능하고 기존의 library를 별다른 수정 없이 쉽게 구현 가능하다.

우리는 ImageNet에서의 포괄적인 실험을 통해 성능 저하 문제를 보여주고 우리의 방법을 평가한다.
- 매우 깊은 residual network는 최적화에 용이하다.<br>
하지만 plain network(단순히 층을 쌓은 network)는 깊이가 증가함에 따라 더 높은 학습 오류를 범한다.
- residual network는 깊이가 증가함에 따라 정확도가 쉽게 증가하고 이전 network보다 더 나은 결과를 제공한다.

이 현상은 CIFAR-10에서도 나타나며, 이는 최적화의 어려움과 이 방법의 효과가 특정 데이터셋에만 국한되지 않음을 알 수 있다. 우리는 이 데이터셋에서 100층이 넘는 모델을 성공적으로 학습했으며, 1000층 이상의 모델도 탐구하였다.

ImageNet Classification 데이터셋에서 우리는 매우 깊은 잔차 네트워크를 통해 우수한 결과를 얻었다. 우리의 152층 잔차 네트워크는 ImageNet에서 제안된 네트워크 중 가장 깊은 네트워크이지만, VGG 네트워크보다 복잡도는 더 낮다. 우리의 앙상블 모델은 ImageNet 테스트 셋에서 3.57%의 top-5 오류율을 기록하며, ILSVRC 2015 분류 대회에서 1위를 차지했다. 이러한 매우 깊은 표현은 다른 인식 작업에서도 우수한 일반화 성능을 보였으며, ImageNet detection, ImageNet localization, COCO detection 및 COCO segmentation에서 1위를 차지했다. 이러한 강력한 증거는 잔차 학습 원리가 일반적임을 보여주며, 이는 다른 비전 및 비비전 문제에도 적용 가능할 것으로 기대된다.

---
## 2. Related Work