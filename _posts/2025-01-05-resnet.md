---
title: "[Paper Review] Deep Residual Learning for Image Recognition"
date: 2025-01-05 10:00:00 +0900
categories: ["Paper Review", "Supervised Learning"]
tags: ["paper review", "resnet"]
use_math: true
---

# Source

> - CVPR 2016 [Paper](https://arxiv.org/pdf/1512.03385), [Git](https://github.com/KaimingHe/deep-residual-networks)<br>
> - Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun

 <br>
> - 16 Nov 2016<br>
> - (UC San Diego, Facebook AI Research)

---
# Contents
## 0. Abstract

깊은 신경망은 학습하기가 더 어렵다. 우리는 기존의 network보다 더 깊은 network를 쉽게 학습하기 위한 residual learning framework를 제안한다. 우리는 layer를 layer inputs을 참조하는 residual function을 학습하도록 명시적으로 재구성한다. 우리는 이러한 residual network가 최적화하기 쉽고 depth를 증가시켜도 정확도를 향상시킬 수 있음을 실험적으로 증명하였다. 우리는 ImageNet dataset에서 152-layer를 사용해 residual net을 평가하였다. 이는 VGG보다 8배 깊지만 Complexity는 더 낮다. 이 residual net을 앙상블한 결과 ImageNet test set에서 3.57%의 error를 달성하였다. 이 결과는 ILSVRC 2015 Classification task에서 1등을 달성하였다. 우리는 또한 CIFAR-10에서 100개의 Layer와 1000개의 Layer에 대한 분석도 진행하였다.
 
Representatoin의 깊이는 많은 Visual recognition task에서 중요한 역할을 한다. 오로지 우리의 매우 깊은 표현력 덕분에 COCO Object Detection dataset에서 28%의 상대적 개선효과를 얻었다. deep residual nets은 우리의 ILSVRC & COCO 2015 대회회의 제출물에 기초가 되었고 ImageNet Detection, ImageNet Localization, COCO Detection, COCO Segmentation에서 1위를 차지하였다.

---

## 1. Introduction

Deep Convolutional 신경망은 Image Classification에서 일련의 돌파구를 만들어 냈다. Deep network는 자연스럽게 low/mid/high level feature와 Classifier를 통합한다. 그리고 feature의 수준은 쌓여진 layer의 수(depth)에 의해 풍부해진다. 최근의 연구는 network의 깊이가 매우 중요하다는 것을 보여주고 ImageNet 데이터셋 대회에서 좋은 기록들은 16~30개의 매우 깊은 모델을 활용하였다. 또한 많은 다른 복잡한 visual recognition task도 깊은 모델을 통해 큰 혜택을 받았다.

깊이의 중요성을 생각해 볼 때, "더 나은 network를 학습시키는 것이 layer를 쌓는 것만큼 쉬울까?" 라는 질문이 생긴다. 이 질문에 답하는데 방해가 되는 문제는 학습 초기에 수렴을 방해하는 gradient vanishing/exploding 문제이다. 그러나 이 문제는 initialization과 batch normalization을 통해 해결되어 왔고, 이는 수십 층을 가진 network가 SGD를 통한 역전파로 수렴할 수 있게 만들었다.

![alt text](/assets/img/post/paper_review/resnet_degrade.png)

더 깊은 network가 수렴하기 시작하면서 성능 저하 문제(degradation, underfitting을 말하는 듯함)가 발생하였다. network의 깊이가 증가하면서 정확도는 saturated되었고, 성능 저하가 급격히 발생하였다. 놀랍게도 이러한 성능 저하는 overfitting때문에 발생한 것은 아니고 적당히 깊은 모델에 layer를 더 추가하는 것은 더 높은 training error를 발생시켰다. 위의 그림은 이의 전형적인 예시이다.

훈련 과정에서 성능 저하는 모든 system이 동일하게 최적화하기 쉬운 것은 아니라는 것을 나타낸다. 얕은 architecture와 여기에 layer를 추가한 깊은 architecture를 생각해보자. (얕은 모델에서 학습된 layer들을 그대로 복사하고 이 위에 identity mapping layer를 추가한다.) 이 새롭게 구성된 해결책은 깊은 모델이 얕은 모델보다 더 높은 training error를 만들어서는 안된다는 것을 의미한다. 그러나 실험에 따르면 현재 기존의 최적화 방법들은 이론적으로 존재하는 최적의 해를 찾을 수 없다는 것을 보여준다.

이 논문에서 우리는 deep residual learning framework를 통해 성능 저하 문제를 다룰 것이다. 