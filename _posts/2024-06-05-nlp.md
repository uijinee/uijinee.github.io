---
title: "10. Natural Language Processing"
date: 2024-06-05 22:00:00 +0900
categories: ["Artificial Intelligence", "Deep Learning(Basic)"]
tags: ["nlp"]
use_math: true
---

<!--
### Bag-of-Words Model

### N-Gram Model

- Fixed Size window를 사용해야 해서 prediction위치를 유동적으로 설정할 수 없다.

### Grammar

### Parsing
-->

# 2. In Deep Learning

## 1. Word Embedding

**Sparse Representation**

| | One-hot Vector | N-Gram Representation |
|:---:| --- | --- |
| 설명 | N개의 단어를 N차원 벡터로 표현 | N-1개의 단어에 기반해<br> 다음에 나올 word의 확률을 구하는 것 |
| 예시 |  word = {'R':[1, 0, 0],<br>$\quad \qquad$ 'G':[0, 1, 0],<br>$\quad \qquad$ 'B': [0, 0, 1]}` | $P(w \vert \text{I am}) = \frac{count(\text{I am }w)}{count(\text{I am})}$<br><br>$w = \text{handsome, cute, cold, ...}$ |
| 단점 | 비슷한 단어끼리 묶을 수 없음 | 여전히 Sparse한 데이터<br> N의 선택에 의해 Trade-off 발생 |

Sparse Representation은 많은 저장공간을 필요로 하기 때문에, <br>
보통 모델 학습을 위해서는 Dense Representation이 필요하다.<br>
_(ex. 강아지 = [0.2, 1.8, 1.1, -2.1, 1.1, 2.8, ...])_

이에 표현 방법을 Dense Representation으로 바꾸어주는 Word Embedding이 반드시 필요하고,<br>이로써 얻을 수 있는 이점은 다음과 같다.
<br>
- Low Dimensional
- 비슷한 단어들끼리 가깝게 표현 가능
- Vector Arithmetic<br>
  _(France - Paris = Greece - Athens)_

Deep Learning을 통해 Word Embedding을 수행할 수 있는데, 이는 주로 Classification이나 Prediction과 같은 특정 Task를 수행하면서 생성된 Weight들을 사용하게 된다.

### 1) Bidirectional RNN

![alt text](/assets/img/post/deeplearning_basic/bidirectional_rnn.png)

> **ⅰ.** Sparse Representation으로 표현된 단어를 Embedding Lookup 모듈에 넣는다.
>
> **ⅱ.** 이 결과를 Bidirectional RNN과 FeedFoward Network를 사용해 해당 단어의 Class(품사)를 예측한다.
>
> **ⅲ.** 이 Embedding Lookup 모듈들을 모아 Table을 만든 후 Word Embedding으로 사용한다.

### 2) Contextual Representations

![alt text](/assets/img/post/deeplearning_basic/contextual_representation.png)

> **ⅰ.** Sparse Representation으로 표현된 단어를 Embedding Lookup 모듈에 넣는다.
>
> **ⅱ.** 이 결과를 RNN과 FeedFoward Network를 사용해 다음에 나올 단어를 예측한다.
>
> **ⅲ.** 이 Embedding Lookup 모듈들을 모아 Table을 만든 후 Word Embedding으로 사용한다.
>
> --- 
> **문제점**
>
> 일방향 RNN을 사용하기 때문에 앞뒤 단어를 고려할 수 없다.<br>
> 때문에 다음과 같은 상황에서 문제가 발생한다.
>
> - 다의어
> - 단어의 뉘앙스
> - 관용구
>
> 그렇다고 양방향 RNN을 사용할 경우 다음에 나올 단어를 미리 알 수 있기 때문에 학습이 제대로 되지 않는다.

### 3) Masked Language Model

![alt text](/assets/img/post/deeplearning_basic/masked_language_model.png)

> **ⅰ.** Sparse Representation으로 표현된 단어를 Embedding Lookup 모듈에 넣는다.<br>
>
> **ⅱ.** 이 결과를 BidirectinaRNN과 FeedFoward Network를 사용해 다음에 나올 단어를 예측한다.
> $\quad \rightarrow$ 이때, 예측하고자 하는 부분에 대해서는 Mask를 씌워 양 방향의 문맥을 고려해 추측하도록 한다.
>
> **ⅲ.** 이것을 각 단어별로 수행한다.

---
## 2. Model

### 1) Bert

![alt text](/assets/img/post/deeplearning_basic/bert.png)

> #### Task
> 
> - Random하게 가려진 단어 예측
> - Next Sentence prediction
>
> ---
>
> ⅰ. Transformer를 사용한 Masked Language Model을 통해 위의 두 Task를 순차적으로 학습
>
> ⅱ. Pretrained된 Bert를 Finetuning하여 Classification등의 Downstream Task수행
>
> $\Rightarrow$ 양방향 문서표현(임베딩) 모델

### 2) GPT

![alt text](/assets/img/post/deeplearning_basic/gpt_bert.png)

> N-Gram문제를 Transformer를 사용하여 푸는 것.
>
> $\Rightarrow$ 단방향 문장 생성 모델
>
> ---
>
> GPT1: Unsupervised + Fine Tunning<br>
> GPT2: Unsupervised<br>
> GPT3: Unsupervised + Incontext Learning<br>
> GPT4: Unsupervised + Multimodal<br>