---
title: "TODO list"
date: 2024-12-18 12:00:00 +0900
categories: ["Paper Review", "Supervised Learning"]
tags: ["paper review"]
use_math: true
---

[The Road Less Scheduled](https://arxiv.org/pdf/2405.15682)

ImageBind: One Embedding Space To Bind Them All
Group ViT

SSL 표준
- [moco1234, simclr]
- [SwAV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments](https://arxiv.org/pdf/2006.09882)
- [Simsiam: Exploring Simple Siamese Representation Learning](https://arxiv.org/pdf/2011.10566)
- [CPCv2: Data-Efficient Image Recognition with Contrastive Predictive Coding](https://arxiv.org/pdf/1905.09272v3)
- [CPC: Representation Learning with Contrastive Predictive Coding](https://arxiv.org/pdf/1807.03748)

Transformer Scaling방법들
- [Scaling vision transformers](https://arxiv.org/pdf/2106.04560)
- [OpenCLIP: Robust fine-tuning of zero-shot models](https://arxiv.org/pdf/2109.01903)

Transformer의 Token 변형
- [Memory Transformer](https://arxiv.org/pdf/2006.11527)
- [Recurrent Memory Transformer](https://arxiv.org/pdf/2207.06881)
- [Fine-tuning Image Transformers using Learnable Memory](https://arxiv.org/pdf/2203.15243)

Transformer Positional Encoding
- [NTK: ]
- [푸리에]

Transformer의 Complexity
- [linformer]






Detection
- [Localizing Objects with Self-Supervised Transformers and no Labels](https://arxiv.org/pdf/2109.14279)
- [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/pdf/2106.00666)
- [ViDT: An Efficient and Effective Fully Transformer-based Object Detector](https://arxiv.org/pdf/2110.03921)