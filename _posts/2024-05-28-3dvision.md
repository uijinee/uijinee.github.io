---
title: "4. 3D Computer Vision"
date: 2024-05-28 22:00:00 +0900
categories: ["Artificial Intelligence", "Computer Vision"]
tags: ["3d", "3d vision"]
use_math: true
---

# 3D Computer Vision

2D Computer Vision의 표현 방법은 하나밖에 존재하지 않았다.<br>
하지만, 3D Computer Vision의 표현 방법은 매우 다양하다.

따라서 이를 먼저 알아보고 각 표현 방법들을 다루는 방법들을 알아보자.

## 1. BackGround

### 1) Coordinate

> #### 좌표계
>
> | 월드 좌표계 | 카메라 좌표계 | 픽셀 좌표계 |
> | --- | --- | --- |
> | 물체의 위치를 표현할 때<br> 기준으로 삼는 좌표계<br> _(임의로 설정 가능)_ <br> | 1. 카메라의 초점을 원점으로한 좌표계<br>2. 방향<br>　ⅰ. 원점: 카메라의 초점<br>　ⅱ. x축: 카메라의 오른쪽<br>　ⅲ. y축: 카메라의 아래쪽<br>　ⅳ. z축: 카메라의 정면 <br>| 1. 투영된 이미지의 좌표계<br>2. 방향<br>　ⅰ. 원점: 이미지의 왼쪽 상단<br>　ⅱ. x축: 카메라의 오른쪽<br>　ⅱ. y축: 카메라의 아래쪽 <br> |
>
>  &#8251; 정규 좌표계: 카메라 내부 파라미터의 영향이 없을 경우 투영되는 공간에 대한 좌표계
>
> ---
> #### 파라미터
>
> ![alt text](/assets/img/post/computer_vision/camera_coordinate.png)
>
> | | 외부 파라미터 행렬 | 내부 파라미터 행렬 |
> |:---:| --- | --- | 
> | 변환 | 월드좌표계 $\rightarrow$ 카메라 좌표계 | 카메라 좌표 $\rightarrow$ 카메라 영상의 픽셀값<br> _(카메라 내부의 기계적인 셋팅)_ |
> | 요소 | 1. 회전이동<br>2. 평행이동 | 1. 초점거리: 렌즈의 중심과 CCD/CMOS와의 거리<br>2. 주점: 렌즈의 중심에서 이미지 센서에 수직으로<br>$\qquad \quad$ 내린 점의 영상픽셀좌표<br> 3. 비대칭 계수: 이미지 센서의 y축이 기울어진 정도<br> 4. 렌즈왜곡<br>5. 영상의 중심값<br>6. 이미지 센서의 Aspect Ratio<br>7. 이미지센서의 Skew Factor<br> |
>
> _(렌즈왜곡 모델: 방사형 렌즈왜곡, 접선형 렌즈왜곡)_
> 

### 2) Representation

> | Depth Map | Voxel Grid | PointCloud |
> | --- | --- | --- |
> | ![alt text](/assets/img/post/computer_vision/depth_map.png) | ![alt text](/assets/img/post/computer_vision/voxel.png)<br>![alt text](/assets/img/post/computer_vision/3dcnn.png) | ![alt text](/assets/img/post/computer_vision/point_cloud.png) | 
> | 카메라와 픽셀사이의 거리 | 3D Grid로 표현 | Point들의 "집합"으로 표현<br> _(Volume: X, Location: O)_ |
> | | (+) 개념적으로 이해하기 쉽다| (+) 적은 수의 점으로 구조 표현 가능 |
> | | (-) detail한 표현을 위해서는<br>$\;\;\;$ 메모리가 많이 필요하다.<br>$\;\;\;$_(3D Kernel $\rightarrow$ 3D CNN 사용)_| (-) Surface표현 불가능<br> (-) 새로운 Loss가 있어야 함<br>$\;\;\;$ _(점들의 "집합"이기 때문)_ |
>
> | Mesh | Implicit Surface |
> | --- | --- |
> | ![alt text](/assets/img/post/computer_vision/mesh.png) | ![alt text](/assets/img/post/computer_vision/implicit_surface.png) |
> | "Vertics"와 "Face"로 이루어진 삼각형들의 "집합"<br> ● Vertics: 삼각형의 모서리<br> ● Face: 삼각형의 면 | |
> | (+) Computer Graphic에서 주로 사용하는 방법<br> (+) Detail한 표현이 필요한 부분은 Face를<br> $\quad$ 더 사용하므로써 Adaptive한 표현 가능<br> (+) UV Map같은 것을 활용해 Color, Texture 같은<br> $\quad$ 추가적인 정보도 표현 가능 | |
> | (-) Nerual Nets에서는 처리하기 쉽지 않음<br> $\quad$ _(Graph Convolution)_ | |

---
## 2. Depth Estimation

### 1) Stereo Matching

### 2) Monocular Depth Estimation
 
| 방법 | 문제점 |
| --- | --- |
| ![alt text](/assets/img/post/computer_vision/depth_segmentation.png) | ![alt text](/assets/img/post/computer_vision/monocular_depth_problem.png) |
| Segmentation모델과 비슷하게 Pixel별로<br> Depth를 Regression하면 된다. _(ex. UNet)_ | **Scale/Depth Ambiguity**<br>작고 가까운 물체와 크고 멀리있는 물체를 구분할 수 없다.<br> 때문에 하나의 이미지로는 Relative Depth만 구할 수 있다. |

> #### [Paper1: DPT](https://arxiv.org/pdf/2103.13413)
> 
> Vision Transformers for Dense Prediction이라는 논문에서는 다음과 같은 방법을 제안한다.
> 
> | Architecture | Abstract |
> | --- | --- |
> | ![alt text](/assets/img/post/computer_vision/dpt_architecture.png) | 기존의 Dense Prediction에서 좋은 성능을 내는 모델들은<br> 다음 두가지의 특징을 갖는다.<br> ● Encoder-Decoder 구조<br> ● CNN Backbone<br>이때, CNN Backbone의 경우 DownSampling을 통해<br> 다양한 Scale의 Feature를 추출한다. 이때, 기존의 정보를<br> 잃을 수 있지만 Segmentation에서는 이를 어느 정도<br> 해결할 수 있었다. 하지만 높은 해상도와 Detail이<br> 요구되는 Dense Prediction에서는<br> 이 **<u>DownSampling과정은 적합하지 않다.</u>**<br><br> $\Rightarrow$즉, DownSampling이 일어나지 않는 ViT를 사용해보자|
> 
> 이 Architecture는 Detection의 FPN과 비슷하게 동작하게 된다.
> 
> | Reassemble Layer | Fusion Layer |
> | --- | --- |
> | ![alt text](/assets/img/post/computer_vision/dpt_reassemble.png) | ![alt text](/assets/img/post/computer_vision/dpt_fusion.png) |
> | ⅰ. Read<br>$\quad$: $N_p+1$개의 토큰을 $N_p$개의 토큰으로 변환 <br>ⅱ. Concatenate<br> $\quad$ :$\frac{H}{p} \times \frac{W}{p} \times D$의 하나의 Block으로 변환<br>ⅲ. Resample<br>$\quad$: Output Size와 맞추기 위한 Upsampling과정 | RefineNet기반 Feature Fusion Block<br><br> UNet구조와 비슷하게 Feature Map을 결합하고<br> 점진적으로 2배씩 Upsampling|
>
> ---
> #### [Paper2: Depth Anything](https://arxiv.org/pdf/2401.10891)

---
## 3. Point Cloud

### 1) Classification

> #### [Paper1: PointNet](https://arxiv.org/pdf/1612.00593)
>
> | Architecture | Abstract |
> | --- | --- |
> | ![alt text](/assets/img/post/computer_vision/pointnet.png) | Point Cloud같은 데이터는 다음의 단점이 존재한다. <br> ● Irregular<br> ● Unordered<br>이 때문에 학습이 쉽지 않고, Voxel로 바꿀 경우<br> 데이터의 부피가 매우 커진다.<br><br> $\Rightarrow$ Raw Point Cloud를 Input으로 하는 모델 제안 |
> 

### 2) 3D Reconstruction

> #### [Paper1: Point Set Generation Network(PSGN)](https://arxiv.org/pdf/1612.00603)
> 
> | Architecture | Abstract |
> | --- | --- |
> | ![alt text](/assets/img/post/computer_vision/pointset_generation.png) | 하나의 2D Image를 통해 3D Point Cloud를<BR> 생성하는 모델로 기존의 Volumetric Grid나<br> Collection of Image방식은 다음의 단점이 존재한다.<br> ● 기하학적 변환시 3D형태의 불변성 모호<BR>  ● 이외에도 다양한 문제 존재<br><br>$\Rightarrow$ Point Cloud를 생성하는 방식의 모델 제안 |
>
> 이때, 생성된 Point Cloud는 순서가 존재하지 않는 "집합"이므로 집합간의 차이를 비교할 수 있는 새로운 방식의 Loss가 필요하다.
>
> **Chamfer Distance**
>
> $$
> d_{CD}(S1, S2) = \sum \limits_{x \in S_1} \min \limits_{y \in S_2} \Vert x-y \Vert_2^2 + \sum \limits_{y \in S_2} \min \limits_{y \in S_1} \Vert x-y \Vert_2^2
> $$
>
> - $\sum \limits_{x \in S_1} \min \limits_{y \in S_2} \Vert x-y \Vert_2^2$<br>
>   ![alt text](/assets/img/post/computer_vision/chamfer_distance.png)<br>
>   : 각 예측값$x$에 대해, 정답값$y \in S_2$중 Nearest Neighbor와의 거리$d_i$를 구하고 $\sum \limits_i d_i$
>
> - $\sum \limits_{y \in S_2} \min \limits_{y \in S_1} \Vert x-y \Vert_2^2$<br>
>   ![alt text](/assets/img/post/computer_vision/chamfer_distance(2).png)<br>
>   : 각 정답값$y$에 대해, 예측값$x \in S_1$중 Nearest Neighbor와의 거리$d_i$를 구하고 $\sum \limits_i d_i$

---
## 4. Predicting Meshes

### 1) 3D Reconstruction

> #### [Paper1: Pixel2Mesh](https://arxiv.org/pdf/1804.01654)
>
> | **Architecture** | ![alt text](/assets/img/post/computer_vision/pixel2mesh.png) |
> | **Abstract** | 기존의 방식은 Multi-View Geometry(MVG)를 기반으로 연구되었다.<br> 하지만 이 방식은 다음과 같은 문제를 갖는다. <br> ● MVG가 제공할 수 있는 범위에 한계가 존재<br>$\;\;$_(보이지 않는 곳 표현 불가)_ <br> ● Reconstruction하고자 하는 객체의 외관에 의해 한계 존재<br>$\;\;$_("투명"하거나 "빛을 반사"하는 물체, "Textureless" 물체는 Reconstruction불가)_<br><br> 이를 해결하기 위해 Mesh를 직접 합성하는 대신, 타원체를 점진적으로<br> 대상 3D Mesh로 변형하는 방법을 제안 _(Corse-To-Fine 전략)_ |
> 
> $$
> \Updownarrow
> $$
> 
> | Graph Convolution | Vertex-Aligned Feature |
> | --- | --- |
> | ![alt text](/assets/img/post/computer_vision/graph_convolution.png) | ![alt text](/assets/img/post/computer_vision/vaf.png) |
> | $$f_i' = W_0f_i + \sum \limits_{j \in N(i)} W_1 f_j$$ <br><br>  Mesh는 삼각형의 Vertex로 이루어져 있기 때문에,<br> 하나의 Vertex의 움직임은 인접한 Vertex에<br> 영향을 주어야 한다.<br><br> 이러한 특징을 반영하여 새롭게 구성한 Layer가<br> Graph Convolution Layer이고 위의 Architecture는<br> 이 Stack of Graph Convolution Layer이다. | ⅰ. 2D 이미지에서 Feature Map 추출<br>ⅱ. 추출된 Feature Map을 3D 모델에 매칭<br>_(3D모델을 2D이미지 평면에 투영 후 매칭)_<br><br>ⅲ. 각 정점에는 이제 해당 위치와 대응되는<br>$\quad$ Image의 Feature를 갖게 됨 |
>
> $$
> \Updownarrow
> $$
>
> **Loss Function**
> 
>  이제 Predict한 Mesh와 Ground Truth Mesh를 비교하는데에는 한가지 문제점이 발생한다. 같은 Shape에 대해 여러가지 표현 방법이 존재하기 때문이다.
> 
> | ![alt text](/assets/img/post/computer_vision/mesh_loss.png) | 이를 위해 다음과 같은 방법을 사용한다.<br> ⅰ. Ground Truth Mesh에 Point를 Sampling하여<br>$\quad$ Point Cloud로 만든다.<br><br> ⅱ. Predicted Mesh에 Point를 Sampling하여<br> $\quad$ Point Cloud로 만든다. <br><br> ⅲ. Champer Distance를 사용해 두 Point Cloud간의 Loss를 계산한다. |
> 
> 하지만 이 방법에는 다음과 같은 문제가 존재한다.
> - Train시에 Online으로 Sampling해야 하므로 이를 효율적으로 할 방법이 필요하다.
> - Sampling을 통한 Backpropagation이 필요하다
>
> ---
> #### [Paper2: Mesh R-CNN](https://arxiv.org/pdf/1906.02739)
>
> | Pipeline | |
> |:---:| --- |
> | ![alt text](/assets/img/post/computer_vision/mesh_rcnnarchitecture.png) <br> ![alt text](/assets/img/post/computer_vision/mesh_rcnnpipeline.png) | 이전에 공부했던 Mask R-CNN은 BBox, Label,<br> Segmentation Map을 찾아주었다.<br><br> Mesh R-CNN은 여기에 Mesh Head를 추가하여<br> 3D Mesh도 예측할 수 있도록 만든 모델이다.<br><br> 이때, 특이한 점은 바로 Mesh를 예측하지 않고,<br> Voxel을 예측한 후 이를 바탕으로 Mesh를<br> Predict하도록 설계한 점이다. |
> 

---
## 5. Implicit Representation

위에서 살펴보았던 모든 방식은 3D Shape를 Point들로 예측하는 방식, 즉, *Explicit Representation*이었다.<br>
이 방식은 Memory도 많이 사용할 뿐 아니라 Discrete하다는 단점이 있다.

여기서는 3D Shape를 함수로 표현하는 방식인 **Implicit Representation**을 알아볼 예정이다.<br>
함수로 예측하기 위해서는 어떤 좌표값을 Input으로 넣었을 때 그 좌표에 Object의 **<u>Inside인지 Outside인지</u>**에 대한 확률을 Output으로 출력하면 된다.

### 1) [Nerf](https://arxiv.org/pdf/2003.08934)

![alt text](/assets/img/post/computer_vision/nerf_problem.png)

Nerf는 Camera Parameter를 알고 있을 때, 같은 장면에 대한 여러 이미지를 통해 새로운 각도에서의 ViewPoint를 찾아내는 문제를 다루고 있다.

> | Pinhole Camera Model | Radiance Field |
> | --- | --- |
> | ![alt text](/assets/img/post/computer_vision/pinholecamera.png) | ![alt text](/assets/img/post/computer_vision/radience_field.png) |  
> | Pinhole을 통해 물체를 인식하는 모델<br><br> 카메라의 기본적인 원리를 나타내고 있고, <br> 물체가 기록되는 과정을 알 수 있다.  | 우리가 추출할 수 있는 정보는 다음과 같다.<br> $\quad$ⅰ) 색깔(Emit) <br> $\quad$ⅱ) 불투명도(Opaque, Occlusion)<br>이 정보들을 공간내의 모든 점들에 대해<br> 추출한 결과를 Radiance Field라고 한다.<br><br> ex) 빈공간 $\qquad \qquad \quad \;\;$ 물체<br>$\quad$ ▷ Emit($c$) = "no" $\qquad$ ▷ Emit($c$) = "Red"<br>$\quad$ ▷ Opaque($\sigma$) = 0 $\qquad$▷ Opaque($\sigma$) = 1 |
> 
> ---
> #### Volume Rendering
>
> 위의 Radience Field를 하나의 함수로 모델링하는 것을 Volume Rendering이라고한다.
> 
> ![alt text](/assets/img/post/computer_vision/volume_rendering.png)
>
> direction이 $r(t) = \mathbf{o} + t\mathbf{d}$라고 할 때 Volume과 Color는 다음과 같이 표현된다.
> 
> - Volume Density<br>
>   : 점 $p$에서의 밀도, 이 점이 얼마나 불투명한지 나타내는 값<br>
>   $\Rightarrow \sigma(\mathbf{P}) \in [0, 1]$
>
> - Color<br>
>   : 점 p에서 방향 d로 방출되는 색상<br>
>   $\Rightarrow c(\mathbf{p}, \mathbf{d}) \in [0, 1]^3$ 
> 
> $$
> \Downarrow
> $$
> 
> **Volume Rendering Equation**
>
> ![alt text](/assets/img/post/computer_vision/volume_rendering(2).png)
>
> $$
> C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t))c(\mathbf{r}(t), \mathbf{d}) dt \approx \sum \limits_{i=1}^N T_i(1 - e^{-\sigma_i \delta_i})\mathbf{c}_i \\
> T(t) = e^{-\int_{t_n}^t \sigma(\mathbf{r}(s)) ds} \approx e^{-\sum \limits_{j=1}^{i-1} \sigma_j \delta_j}
> $$
>
> - $T(t)$: Transmittance, 현재 Point에서 출발한 빛이 Camera에 얼마나 도달하는지
> - $\sigma(\mathbf{r}(t))$: Opacity, 현재 Point의 불투명도
> - $c(\mathbf{r}(t), \mathbf{d})$: Color, 현재 Point에서 Camera에 무슨색의 빛을 방출하는지
>
> 우리는 이 식을 Sampling을 위해 Approximate하여 사용한다.
>
> 이제 Sensor의 r방향에서의 Pixel값 $C(\mathbf{r})$에 대한 모델링을 하였으니 이를 활용하여 Deep Neural Network를 학습하면 된다.<br>
> _(**Input**= $p,d \quad \Rightarrow \quad$ **Output** $\sigma(\mathbf{r}), c(\mathbf{p}, \mathbf{d})$)_
> 
> ---
> #### Architecture
>
> ![alt text](/assets/img/post/computer_vision/nerf_architecture.png)
> 