---
title: "3. 3D Computer Vision"
date: 2024-05-28 22:00:00 +0900
categories: ["Artificial Intelligence", "Deep Learning(Advanced)"]
tags: ["3d", "3d vision"]
use_math: true
---

# 3D Computer Vision

2D Computer Vision의 표현 방법은 하나밖에 존재하지 않았다.<br>
하지만, 3D Computer Vision의 표현 방법은 매우 다양하다.

따라서 이를 먼저 알아보고 각 표현 방법들을 다루는 방법들을 알아보자.

## 1. BackGround

### 1) Coordinate

> #### 좌표계
>
> | 월드 좌표계 | 카메라 좌표계 | 픽셀 좌표계 |
> | --- | --- | --- |
> | 물체의 위치를 표현할 때<br> 기준으로 삼는 좌표계<br> _(임의로 설정 가능)_ <br> | 1. 카메라의 초점을 원점으로한 좌표계<br>2. 방향<br>　ⅰ. 원점: 카메라의 초점<br>　ⅱ. x축: 카메라의 오른쪽<br>　ⅲ. y축: 카메라의 아래쪽<br>　ⅳ. z축: 카메라의 정면 <br>| 1. 투영된 이미지의 좌표계<br>2. 방향<br>　ⅰ. 원점: 이미지의 왼쪽 상단<br>　ⅱ. x축: 카메라의 오른쪽<br>　ⅱ. y축: 카메라의 아래쪽 <br> |
>
>  &#8251; 정규 좌표계: 카메라 내부 파라미터의 영향이 없을 경우 투영되는 공간에 대한 좌표계
>
> ---
> #### 파라미터
>
> ![alt text](/assets/img/post/deeplearning_advanced/camera_coordinate.png)
>
> | | 외부 파라미터 행렬 | 내부 파라미터 행렬 |
> |:---:| --- | --- | 
> | 변환 | 월드좌표계 $\rightarrow$ 카메라 좌표계 | 카메라 좌표 $\rightarrow$ 카메라 영상의 픽셀값<br> _(카메라 내부의 기계적인 셋팅)_ |
> | 요소 | 1. 회전이동<br>2. 평행이동 | 1. 초점거리: 렌즈의 중심과 CCD/CMOS와의 거리<br>2. 주점: 렌즈의 중심에서 이미지 센서에 수직으로<br>$\qquad \quad$ 내린 점의 영상픽셀좌표<br> 3. 비대칭 계수: 이미지 센서의 y축이 기울어진 정도<br> 4. 렌즈왜곡<br>5. 영상의 중심값<br>6. 이미지 센서의 Aspect Ratio<br>7. 이미지센서의 Skew Factor<br> |
>
> _(렌즈왜곡 모델: 방사형 렌즈왜곡, 접선형 렌즈왜곡)_
> 

### 2) Representation

> | Depth Map | Voxel Grid | PointCloud |
> | --- | --- | --- |
> | ![alt text](/assets/img/post/deeplearning_advanced/depth_map.png) | ![alt text](/assets/img/post/deeplearning_advanced/voxel.png)<br>![alt text](/assets/img/post/deeplearning_advanced/3dcnn.png) | ![alt text](/assets/img/post/deeplearning_advanced/point_cloud.png) | 
> | 카메라와 픽셀사이의 거리 | 3D Grid로 표현 | Point들의 "집합"으로 표현<br> _(Volume: X, Location: O)_ |
> | | (+) 개념적으로 이해하기 쉽다| (+) 적은 수의 점으로 구조 표현 가능 |
> | | (-) detail한 표현을 위해서는<br>$\;\;\;$ 메모리가 많이 필요하다.<br>$\;\;\;$_(3D Kernel $\rightarrow$ 3D CNN 사용)_| (-) Surface표현 불가능<br> (-) 새로운 Loss가 있어야 함<br>$\;\;\;$ _(점들의 "집합"이기 때문)_ |
>
> | Mesh | Implicit Surface |
> | --- | --- |
> | ![alt text](/assets/img/post/deeplearning_advanced/mesh.png) | ![alt text](/assets/img/post/deeplearning_advanced/implicit_surface.png) |
> | "Vertics"와 "Face"로 이루어진 삼각형들의 "집합"<br> ● Vertics: 삼각형의 모서리<br> ● Face: 삼각형의 면 | |
> | (+) Computer Graphic에서 주로 사용하는 방법<br> (+) Detail한 표현이 필요한 부분은 Face를<br> $\quad$ 더 사용하므로써 Adaptive한 표현 가능<br> (+) UV Map같은 것을 활용해 Color, Texture 같은<br> $\quad$ 추가적인 정보도 표현 가능 | |
> | (-) Nerual Nets에서는 처리하기 쉽지 않음<br> $\quad$ _(Graph Convolution)_ | |

---
## 2. Depth Estimation

### 1) Stereo Matching

### 2) Monocular Depth Estimation
 
| 방법 | 문제점 |
| --- | --- |
| ![alt text](/assets/img/post/deeplearning_advanced/depth_segmentation.png) | ![alt text](/assets/img/post/deeplearning_advanced/monocular_depth_problem.png) |
| Segmentation모델과 비슷하게 Pixel별로<br> Depth를 Regression하면 된다. _(ex. UNet)_ | **Scale/Depth Ambiguity**<br>작고 가까운 물체와 크고 멀리있는 물체를 구분할 수 없다.<br> 때문에 하나의 이미지로는 Relative Depth만 구할 수 있다. |

> #### [Paper1: DPT](https://arxiv.org/pdf/2103.13413)
> 
> Vision Transformers for Dense Prediction이라는 논문에서는 다음과 같은 방법을 제안한다.
> 
> | Architecture | Abstract |
> | --- | --- |
> | ![alt text](/assets/img/post/deeplearning_advanced/dpt_architecture.png) | 기존의 Dense Prediction에서 좋은 성능을 내는 모델들은<br> 다음 두가지의 특징을 갖는다.<br> ● Encoder-Decoder 구조<br> ● CNN Backbone<br>이때, CNN Backbone의 경우 DownSampling을 통해<br> 다양한 Scale의 Feature를 추출한다. 이때, 기존의 정보를<br> 잃을 수 있지만 Segmentation에서는 이를 어느 정도<br> 해결할 수 있었다. 하지만 높은 해상도와 Detail이<br> 요구되는 Dense Prediction에서는<br> 이 **<u>DownSampling과정은 적합하지 않다.</u>**<br><br> $\Rightarrow$즉, DownSampling이 일어나지 않는 ViT를 사용해보자|
> 
> 이 Architecture는 Detection의 FPN과 비슷하게 동작하게 된다.
> 
> | Reassemble Layer | Fusion Layer |
> | --- | --- |
> | ![alt text](/assets/img/post/deeplearning_advanced/dpt_reassemble.png) | ![alt text](/assets/img/post/deeplearning_advanced/dpt_fusion.png) |
> | ⅰ. Read<br>$\quad$: $N_p+1$개의 토큰을 $N_p$개의 토큰으로 변환 <br>ⅱ. Concatenate<br> $\quad$ :$\frac{H}{p} \times \frac{W}{p} \times D$의 하나의 Block으로 변환<br>ⅲ. Resample<br>$\quad$: Output Size와 맞추기 위한 Upsampling과정 | RefineNet기반 Feature Fusion Block<br><br> UNet구조와 비슷하게 Feature Map을 결합하고<br> 점진적으로 2배씩 Upsampling|
>
> ---
> #### [Paper2: Depth Anything](https://arxiv.org/pdf/2401.10891)

---
## 3. Point Cloud

### 1) Classification

> #### [Paper1: PointNet](https://arxiv.org/pdf/1612.00593)
>
> | Architecture | Abstract |
> | --- | --- |
> | ![alt text](/assets/img/post/deeplearning_advanced/pointnet.png) | Point Cloud같은 데이터는 다음의 단점이 존재한다. <br> ● Irregular<br> ● Unordered<br>이 때문에 학습이 쉽지 않고, Voxel로 바꿀 경우<br> 데이터의 부피가 매우 커진다.<br><br> $\Rightarrow$ Raw Point Cloud를 Input으로 하는 모델 제안 |
> 

### 2) 3D Reconstruction

> #### [Paper1: Point Set Generation Network(PSGN)](https://arxiv.org/pdf/1612.00603)
> 
> | Architecture | Abstract |
> | --- | --- |
> | ![alt text](/assets/img/post/deeplearning_advanced/pointset_generation.png) | 하나의 2D Image를 통해 3D Point Cloud를<BR> 생성하는 모델로 기존의 Volumetric Grid나<br> Collection of Image방식은 다음의 단점이 존재한다.<br> ● 기하학적 변환시 3D형태의 불변성 모호<BR>  ● 이외에도 다양한 문제 존재<br><br>$\Rightarrow$ Point Cloud를 생성하는 방식의 모델 제안 |
>
> 이때, 생성된 Point Cloud는 순서가 존재하지 않는 "집합"이므로 집합간의 차이를 비교할 수 있는 새로운 방식의 Loss가 필요하다.
>
> **Chamfer Distance**
>
> $$
> d_{CD}(S1, S2) = \sum \limits_{x \in S_1} \min \limits_{y \in S_2} \Vert x-y \Vert_2^2 + \sum \limits_{y \in S_2} \min \limits_{y \in S_1} \Vert x-y \Vert_2^2
> $$
>
> - $\sum \limits_{x \in S_1} \min \limits_{y \in S_2} \Vert x-y \Vert_2^2$<br>
>   ![alt text](/assets/img/post/deeplearning_advanced/chamfer_distance.png)<br>
>   : 각 예측값$x$에 대해, 정답값$y \in S_2$중 Nearest Neighbor와의 거리$d_i$를 구하고 $\sum \limits_i d_i$
>
> - $\sum \limits_{y \in S_2} \min \limits_{y \in S_1} \Vert x-y \Vert_2^2$<br>
>   ![alt text](/assets/img/post/deeplearning_advanced/chamfer_distance(2).png)<br>
>   : 각 정답값$y$에 대해, 예측값$x \in S_1$중 Nearest Neighbor와의 거리$d_i$를 구하고 $\sum \limits_i d_i$

---
## 4. Predicting Meshes

### 1) 3D Reconstruction

> #### [Paper1: Pixel2Mesh](https://arxiv.org/pdf/1804.01654)
>
> | **Architecture** | ![alt text](/assets/img/post/deeplearning_advanced/pixel2mesh.png) |
> | **Abstract** | 기존의 방식은 Multi-View Geometry(MVG)를 기반으로 연구되었다.<br> 하지만 이 방식은 다음과 같은 문제를 갖는다. <br> ● MVG가 제공할 수 있는 범위에 한계가 존재<br>$\;\;$_(보이지 않는 곳 표현 불가)_ <br> ● Reconstruction하고자 하는 객체의 외관에 의해 한계 존재<br>$\;\;$_("투명"하거나 "빛을 반사"하는 물체, "Textureless" 물체는 Reconstruction불가)_<br><br> 이를 해결하기 위해 Mesh를 직접 합성하는 대신, 타원체를 점진적으로<br> 대상 3D Mesh로 변형하는 방법을 제안 _(Corse-To-Fine 전략)_ |
> 
> | Graph Convolution | Vertex-Aligned Feature |
> | --- | --- |
> | ![alt text](/assets/img/post/deeplearning_advanced/graph_convolution.png) | ![alt text](/assets/img/post/deeplearning_advanced/vaf.png) |
> | $$f_i' = W_0f_i + \sum \limits_{j \in N(i)} W_1 f_j$$ <br><br>  Mesh는 삼각형의 Vertex로 이루어져 있기 때문에,<br> 하나의 Vertex의 움직임은 인접한 Vertex에<br> 영향을 주어야 한다.<br><br> 이러한 특징을 반영하여 새롭게 구성한 Layer가<br> Graph Convolution Layer이고 위의 Architecture는<br> 이 Stack of Graph Convolution Layer이다. | ⅰ. 2D 이미지에서 Feature Map 추출<br>ⅱ. 추출된 Feature Map을 3D 모델에 매칭<br>_(3D모델을 2D이미지 평면에 투영 후 매칭)_<br><br>ⅲ. 각 정점에는 이제 해당 위치와 대응되는<br>$\quad$ Image의 Feature를 갖게 됨 |


---
## 5. Implicit Representation

### 1) Nerf